{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Competitive learning on the MNIST database\n",
    "\n",
    "## Learning Outcomes\n",
    "- Understand how competive learning works.\n",
    "- Ability to develop and tune a simple neural network using competitive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "### Competitive learning\n",
    "- In competitive learning, the output neurons comptete among themselves to be activated (fire).\n",
    "- In contrast with the Hebbian learning, here only one neuron (or group of neurons) can be activate at a time.\n",
    "- The output neuron that wins the competitionis is often called winner-take-all neuron.\n",
    "\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture5/icons/fig1.png\" width=\"500\">\n",
    "\n",
    "\n",
    "### Main steps for compatitive learning implementation:\n",
    "1. Generate a set of output neurons with random weights.\n",
    "2. Choose a random input pattern and calculate the activation of each of the output neurons.\n",
    "3. Detect the winner neuron and update only its weights; the weights of the other neurons are not updated:  , where  is the winner neuron,  is the learning rate and  is the input vector.\n",
    "4. Repeat from step 2 until the weights are no longer changing, or change less than a set threshold, or a set maximum number of iterations has been reached.\n",
    "\n",
    "### Some optimisation and tuning methods:\n",
    "1. Normalised input or initial weights.\n",
    "2. Noise addition on the weights.\n",
    "3. Decaying learning rate.\n",
    "4. Lucky learning: update the weights of the losers as well as winners but with a much smaller learning rate.\n",
    "5. Update the winners and neighbouring losers.\n",
    "\n",
    "### K-means clustering\n",
    "Clustering: given a set of datapoints we want to group them based on their similarity. Each one of these groups is called cluster.\n",
    "\n",
    "In k-means clustering the task is to separate our data into k clusters. However specifying the number of clusters k is a common problem in machine learning and there are various different methods for finding the optimal k.\n",
    "\n",
    "The k-means algorithm is the following:\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture5/icons/fig2.png\" width=\"500\">\n",
    "\n",
    "### Competitive learning vs K-means clustering\n",
    "Essentially they are equivalent.\n",
    "\n",
    "Competitive learning leads to clusters since each output neuron after training is firing on a certain input (has learn a specific data pattern).\n",
    "The terms centroid and protorype are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 5: Competitive learning on the MNIST database\n",
    "\n",
    "### Exercise\n",
    "The aim of this lab is to implement the standard competitive learning algorithm on a one layer network and use it to classify the hand written digits set. You may use the max operator to find the winner for each input. Please choose the number of output units such that you can capture all classes and tune the network such that it will learn quickly and result in as few dead units as possible. One suggestion is to add noise to the decision neurons, in combination with an appropriate decaying learning rate, but you are free to apply other techniques. In this example you are able to visualise both data and prototypes, and can easily locate the dead units. The vectors provided are 28x28 images and you can reshape them to see the digits. Propose a method for detecting dead units, without using the visualisation of the prototypes. Note: It is a good and sometimes necessary practice to normalise the weight vector and the data at the beginning of the process.\n",
    "\n",
    "You will need to produce the following :\n",
    "1. A figure showing the average weight change as a function of time. When your network has sufficiently learned from the data? If you implement an on-line version of the rule you may use a moving average to produce a smooth curve of the weight changes through time. Such a curve may be more informative on semi-log or log-log axes.\n",
    "2. A figure of the prototypes and a comment on what they represent. How many prototypes did your network find?\n",
    "3. The correlation matrix of the prototypes. How can you use this information to find similarities between the prototypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters and variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: specify parameters and variables e.g.\n",
    "eta = 0.09; # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Implementation, training and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementation, training and output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
