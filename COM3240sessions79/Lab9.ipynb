{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Intelligence COM3240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script that returns the learnign curve for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def homing_nn(n_trials,learning_rate,eps,gamma):\n",
    "\n",
    "    # Solving homing task with on-policy TD (SARSA)\n",
    "\n",
    "    #n_trials = 1000\n",
    "    n_steps = 50\n",
    "\n",
    "    ## Definition of the environment\n",
    "    N = 3                               #height of the gridworld ---> number of rows\n",
    "    M = 4                              #length of the gridworld ---> number of columns\n",
    "    N_states = N * M                    #total number of states\n",
    "    states_matrix = np.eye(N_states)\n",
    "    N_actions = 4                                           #number of possible actions in each state: 1->N 2->E 3->S 4->W\n",
    "    action_row_change = np.array([-1,0,+1,0])               #number of cell shifted in vertical as a function of the action\n",
    "    action_col_change = np.array([0,+1,0,-1])               #number of cell shifted in horizontal as a function of the action\n",
    "    End = np.array([1, 1])                                  #terminal state--->reward\n",
    "    s_end = np.ravel_multi_index(End,dims=(N,M),order='F')  #terminal state. Conversion in single index\n",
    "\n",
    "    ## Parameters of the model\n",
    "    #gamma = 0.9                        #discounting factor\n",
    "    #learning_rate = 0.5                #constant step-size parameter (learning rate)\n",
    "    #eps = 0.0                          #epsilon-greedy SARSA\n",
    "\n",
    "    ## Rewards\n",
    "    R = 10                              #only when the robot reaches the charger, sited in End state\n",
    "\n",
    "    ## Variables\n",
    "    weights = np.random.rand(N_actions,N_states)\n",
    "    learning_curve = np.zeros((1,n_trials))\n",
    "\n",
    "    ## SARSA\n",
    "\n",
    "    # Start trials\n",
    "    for trial in range(n_trials):\n",
    "\n",
    "        # Initialization\n",
    "        Start = np.array([np.random.randint(N),np.random.randint(M)])   #random start\n",
    "        s_start = np.ravel_multi_index(Start,dims=(N,M),order='F')      #conversion in single index\n",
    "        state = Start                                                   #set current state\n",
    "        s_index = s_start                                               #conversion in single index\n",
    "        step = 0\n",
    "\n",
    "        # Start steps\n",
    "        while s_index != s_end and step <= n_steps:\n",
    "\n",
    "            step += 1\n",
    "            learning_curve[0,trial] = step\n",
    "\n",
    "            input_vector = states_matrix[:,s_index].reshape(N_states,1)         #convert the state into an input vector\n",
    "\n",
    "            #compute Qvalues. Qvalue=logsig(weights*input). Qvalue is 2x1, one value for each output neuron\n",
    "            Q = 1 / ( 1 + np.exp( - weights.dot(input_vector)))    #Qvalue is 2x1 implementation of logsig\n",
    "\n",
    "            #eps-greedy policy implementation\n",
    "            greedy = (np.random.rand() > eps)               #1--->greedy action 0--->non-greedy action\n",
    "            if greedy:\n",
    "                action = np.argmax(Q)                           #pick best action\n",
    "            else:\n",
    "                action = np.random.randint(N_actions)           #pick random action\n",
    "\n",
    "\n",
    "            state_new = np.array([0,0])\n",
    "            #move into a new state\n",
    "            state_new[0] = state[0] + action_row_change[action]\n",
    "            state_new[1] = state[1] + action_col_change[action]\n",
    "\n",
    "            #put the robot back in grid if it goes out. Consider also the option to give a negative reward\n",
    "            if state_new[0] < 0:\n",
    "                state_new[0] = 0\n",
    "            if state_new[0] >= N:\n",
    "                state_new[0] = N-1\n",
    "            if state_new[1] < 0:\n",
    "                state_new[1] = 0\n",
    "            if state_new[1] >= M:\n",
    "                state_new[1] = M-1\n",
    "\n",
    "            s_index_new = np.ravel_multi_index(state_new,dims=(N,M),order='F')  #conversion in a single index\n",
    "\n",
    "            ## TODO update Qvalues. Only if is not the first step\n",
    "\n",
    "            #store variables for sarsa computation in the next step\n",
    "            output = np.zeros((N_actions,1))\n",
    "            output[action] = 1\n",
    "\n",
    "            #update variables\n",
    "            input_old = input_vector\n",
    "            output_old = output\n",
    "            Q_old = Q[action]\n",
    "            r_old = 0\n",
    "\n",
    "            state[0] = state_new[0]\n",
    "            state[1] = state_new[1]\n",
    "            s_index = s_index_new\n",
    "\n",
    "            ## TODO: check if state is terminal and update the weights consequently\n",
    "            if s_index == s_end:\n",
    "                pass\n",
    "\n",
    "\n",
    "    return learning_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function homing_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "homing_nn(1000,alpha,epsilon,gamma)\n",
    "\n",
    "\n",
    "# TODO: average rewards over more runs and plot them with error bars\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
