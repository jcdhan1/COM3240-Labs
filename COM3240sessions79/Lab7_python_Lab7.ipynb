{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Intelligence COM3240\n",
    "## Lab 7: Reinforcement Learning\n",
    "\n",
    "### Learning Outcomes\n",
    "- Understand how reinforcement learning works.\n",
    "- Ability to develop a simple reinforcement learning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    Q\\left( s,a \\right)^{K+1}=Q\\left( s,a \\right)^K + \\eta\\left(r-Q\\left(s,a\\right)^K\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture overview\n",
    "\n",
    "### Reinforcement Learning\n",
    "Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal (Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto).\n",
    "In a typical reinforcement learning scenario a learner (or agent) has to achieve a specific goal by performing correct actions. \n",
    "\n",
    "The 'correctness' of an action is estimaded by state-action pair functions which are called value funtions and mathematically are described by the notation .\n",
    "Alternative, value funtions estimate how good a particular action will be on a given state, under a policy.\n",
    "\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture7/icons/pic1.png\" width=\"500\"/>\n",
    "\n",
    "How do we stimate $Q(s , a)?$ => We **explore**. How do we choose an action? => **We choose the action that maximizes our reward.**\n",
    "\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture7/icons/pic2.png\" width=\"500\"/>\n",
    "\n",
    "Exporation vs. exploitation policies (e.g. $\\varepsilon$-greedy, soft-max, optimistic greedy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 7: Reinforcement Learning\n",
    "\n",
    "### Exercise\n",
    "A monkey is randomly presented with two alternative images. One shows a balloon and the other a bicycle. In front of the monkey, there are two buttons, one Green and one Red. Every time the balloon is presented and the monkey presses the Green button, it receives a bit of juice (reward). Similarly, if the bicycle is presented and the Red button is pressed, it again receives juice. No other combination is rewarded. In this simple setup, your monkey, child or artificial agent can learn a specific mapping (Balloon− >Green) and (Bicycle− >Red) from rewards.\n",
    "\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture7/icons/pic2.png\" width=\"500\"/>\n",
    "\n",
    "1. Write code that implements this scenario. The artificial agent may found itself in two states, ”bicycle” or ”balloon”, with equal probabilities. For this reason, you need to use a random generator (rand). From each of the two states, it can choose two actions Red or Green, based on the Q values of the state-action pair. You can use the update rule for Q values we covered in the class in order to learn the optimal actions. Initialise the Qs randomly between 0 and 1 (rand). You can use a Greedy policy. Pre-define the number of consequent presentations (trials). If in difficulties with setting up variables, please ask the demonstrators for help. Note: In order to make the algorithm work, you need to find appropriate parameters. Try different values for the learning rate.\n",
    "2. Plot the average total reward as a function of the trials. This is the learning curve and successful learning means that the reward increases as the trial number increases (up to a maximum value). If in difficulties with plotting commands, please ask the demonstrators.\n",
    "3. In order to draw conclusions we need to repeat this procedure many times and show an average learning curve. Repeat the above procedure 10 times and plot the average graph with errorbars (Matlab commands mean and errorbar).\n",
    "4. The use of an epsilon-greedy policy would make any difference in this case? Modify your code accordingly and explain your results. What is the best value for epsilon in this case and for which reason?\n",
    "5. The rules of the game change. Initial Q values are set to zero. In case of a wrong response, a small reward of 0.1 is administered. In case of a correct response, a reward of 10 with probability 1/10 is given (i.e. in 1 out of 10 cases). Change your code accordingly. Would you be able to achieve maximum reward with the parameters you have used earlier? If not, how should you change the learning rate and the epsilon parameter?\n",
    "\n",
    "It is possible to implement your algorithm on an Artificial Neural Network (ANN). To do so, you need to create a representation of the two images as an input vector (say a column vector 2x1; for instance [1 0]’ for Ballon and [0 1]’ for bicycle. Then you have to use a 2x2 weight matrix producing a 2x1 output vector. Use this vector as an input to a sigmoid function and call this result your output V (firing rate of the output (action) neurons). Say for instance that position V(1,1) represents the Q value of Green and V(2,1) represents the Q value of Red.\n",
    "\n",
    "In this setup, the Q values are updated in an indirect way by modifying accordingly the synaptic weights. It turns out that a rule allowing us to appropriately modify the connections is a hebbian rule with a reward-dependant factor: $\\Delta w_{i,j} =  \\eta\\left[r-Q(s,i)\\right]x_ix_j^s$ , where $r$ is the reward $Q(s,i)$ the $Q$  value of state $s$ and action $i$ (the selected action), $x_i$ the activity of the neuron that represents the selected action and $x_j^s$ the j-th presynaptic neuron, where $j$ is an index on the elements of the column input vector. \n",
    "_Note_: we assume that  is 1 for the neuron that represents the action we took (and 0 for the others) and NOT its output value.\n",
    "\n",
    "1. Modify your code in order to you use an ANN and plot the learning curve.\n",
    "2. Explore the effect of the learning parameters on the performance of your network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function that returns the rewards for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def monkey_template(nTrials, learningRate, epsilon):\n",
    "\n",
    "    # expected rewards: initially random\n",
    "    Qvalue = np.random.rand(2, 2)\n",
    "    print(Qvalue.shape)\n",
    "    \n",
    "    # actions\n",
    "    Red = 0\n",
    "    Green = 1\n",
    "    \n",
    "    # inputs (states)\n",
    "    Balloon = 0\n",
    "    Bicycle = 1\n",
    "    \n",
    "    # output: rewards for each trial\n",
    "    Rewards = np.zeros((1, nTrials))\n",
    "    \n",
    "    for trial in range(nTrials):\n",
    "        # present a picture / state\n",
    "        Image = np.random.randint(2) # Half times Balloon, half times Bicycle\n",
    "                \n",
    "        # select best current action (Greedy) \n",
    "        ##TODO: turn algorithm into an epsilon-Greedy one\n",
    "        ## (random actions can be selected with a probability = epsilon)\n",
    "       \n",
    "        \n",
    "        \n",
    "        if (np.random.ranf(1) > epsilon):\n",
    "            #Select current action\n",
    "            Action = int(Qvalue[Image, Green] > Qvalue[Image, Red]) # will result Action=1 (Green) if Qvalue(Image,Green) > Qvalue(Image,Red) and Action = 0 (Red)\n",
    "        else:\n",
    "            #Select random action\n",
    "            Action = print(np.random.choice(2,1))\n",
    "        \n",
    "        # Collect reward\n",
    "        if Image == Balloon and Action == Green:\n",
    "            r = 1\n",
    "        elif Image == Bicycle and Action == Red:\n",
    "            r = 1\n",
    "        else:\n",
    "            r = 0\n",
    "\n",
    "        ## TODO: update Q values\n",
    "         Qvalue[Image,Action] = Qvalue[Image,Action] + learningRate * (r - Qvalue[Image,Action])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # store reward\n",
    "        Rewards[0,trial] = r\n",
    "\n",
    "    return Rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script that calls the function `monkey_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# This script calls the function monkey_template (1 run)\n",
    "# The code has been developed as a demo, see if you can improve it.\n",
    "# For instance there is no check whether the parameters are correct or not.\n",
    "\n",
    "\n",
    "# Parameter setup\n",
    "nTrials = 50        # should be integer >0\n",
    "learningRate = 0.5  # should be real, Greater than 0\n",
    "epsilon = 0.0       # Should be real, Greater or Equal to 0; epsilon=0 Greedy, otherwise epsilon-Greedy\n",
    "\n",
    "fontSize=18\n",
    "\n",
    "r = monkey_template(nTrials, learningRate, epsilon)\n",
    "\n",
    "#TODO: average rewards over more runs and plot them with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
