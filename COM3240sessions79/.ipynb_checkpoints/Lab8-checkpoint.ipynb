{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Intelligence COM3240\n",
    "\n",
    "## Lab 8: Reinforcement Learning - Future Rewards\n",
    "\n",
    "### Learning Outcomes\n",
    "- Understand how reinforcement learning works.\n",
    "- Ability to develop a simple reinforcement learning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture overview\n",
    "\n",
    "### Future rewards\n",
    "\n",
    "It is possible for the agent to be rewarded not only when he reached the goal (future reward) but also by performing correct actions towards the goal (immediate reward).\n",
    "\n",
    "There should be a difference between the  and , since future reward cannot be as good as present reward: $Q(s,a)$ and $Q(s', as')$ since future reward cannot be as good as the present reward:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta Q (s,a) &= \\eta\\left[r- \\left(Q(s,a) - Q(s', a') \\right)\\right] \\Rightarrow \\\\\n",
    "\\Delta Q (s,a) &= \\eta\\left[r- \\left(Q(s,a) - \\gamma Q(s', a') \\right)\\right] \n",
    "\\end{align}\n",
    "\n",
    "where $\\gamma$ is a discount factor\n",
    "\n",
    "The SARSA algorithm\n",
    "\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture8/icons/sarsa_algorithm.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 8: Future rewards\n",
    "\n",
    "#### Exercise\n",
    "An agent is presented with two buttons, Green and Red. Reward will be given if it presses first the Red button and then the Green button.\n",
    "1. In a simple diagram show the states for this paradigm.\n",
    "2. Can we use simple update rule (used in the previous Lab) to solve this task? Justify your response. Explain your arguments without resorting to simulations.\n",
    "3. Modify your earlier code of an ANN (developed in the previous Lab) using the SARSA algorithm to model this scenario. Suggested input vectors to represent the three states: [1 0 0]', [0 1 0]', [0 0 1]', but you are free to study alternative (better) implementations. Plot the learning curve.\n",
    "4. In order to draw conclusions we need to repeat this procedure many times and show an average learning curve. Repeat the above procedure 10 times and plot the average graph with errorbars (Matlab commands mean and errorbar).\n",
    "5. Would the use of an epsilon-greedy policy make any difference in this case? Modify your code accordingly and explain your results.\n",
    "6. The rules of the game change. Initial Q values are set to zero. In case of a wrong response, a small reward of 0.1 is administered. In case of a correct response, a reward of 10 with probability 1/10 is given (i.e. in 1 out of 10 cases). Change your code accordingly. Would you be able to achieve maximum reward with the parameters you have used earlier? If not, could you and appropriate parameters to achieve learning? Would the use of an epsilon-greedy policy make any difference in this case?\n",
    "7. What are the main parameters of the model and how do they affect performance? Justify your response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that returns the rewards for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sarsa_monkey_nn(nTrials,learningRate,epsilon,gamma):\n",
    "\n",
    "    # States and Actions\n",
    "    neutral = 0\n",
    "    pressedRed = 1\n",
    "    pressedGreen = 2\n",
    "\n",
    "    Red = 0\n",
    "    Green = 1\n",
    "\n",
    "    nStates = 3\n",
    "    nActions = 2\n",
    "\n",
    "    choices = 2 # number of steps within each trial\n",
    "\n",
    "    # Define input as unit vectors to present to the network --> neutral=(1,0,0), pressedRed=(0,1,0), pressedGreen=(0,0,1)\n",
    "    state = np.eye(nStates)\n",
    "\n",
    "    # Weights matrix, connecting input neurons (state) to output neurons (actions). Initially random\n",
    "    weights = np.random.rand(nActions,nStates)\n",
    "\n",
    "    # Define reward vector (one position for each trial) and initialise it to zero\n",
    "    Rewards = np.zeros((1,nTrials))\n",
    "\n",
    "    # Start the episode\n",
    "    for trial in range(nTrials):\n",
    "\n",
    "        # Each episode starts at neutral --> non of the buttons is pressed\n",
    "        stateIndex = neutral\n",
    "\n",
    "        # Define and initialise at every trial the variables needed for sarsa algorithm\n",
    "        inputOld = np.zeros((nStates,1))\n",
    "        outputOld = np.zeros((nActions,1))\n",
    "        QvalueOld = 0\n",
    "        rOld = 0\n",
    "\n",
    "        # simulate the two step trial\n",
    "        for step in range(choices):\n",
    "\n",
    "            # Convert the initial state into a vector\n",
    "            input_vector = state[:,stateIndex].reshape((3,1))\n",
    "\n",
    "            # Compute Qvalues.  Qvalue = logsig(weights*input). Qvalue is 2x1, one value for each output neuron\n",
    "            Qvalue = 1 / (1 + np.exp(- weights.dot(input_vector)))\n",
    "\n",
    "            # Epsilon-greedy parameter\n",
    "            eGreedy = int(np.random.rand() < epsilon) # with probability epsilon choose action at random; if epsilon = 0 then always choose Greedy\n",
    "\n",
    "            # Implement the policy\n",
    "            if Qvalue[Red] == Qvalue[Green] or eGreedy:\n",
    "                Action = np.random.randint(2) # if Qvalues are the same or epsilon>0 (e-Greedy, choose at random with probability epsilon) choose one at random\n",
    "            else:\n",
    "                Action = int(Qvalue[Green] > Qvalue[Red]) # otherwise choose greedy. Will result Action=1 (Green) if Qvalue(Image,Green)>Qvalue(Imgage,Red)\n",
    "\n",
    "            # Collect Reward\n",
    "            if stateIndex == pressedRed and Action == Green:\n",
    "                r = 1\n",
    "            else:\n",
    "                r = 0\n",
    "\n",
    "            # Rectified output - a binary array with a single non-zero element corresponding to the selected action. This is in order to update the weights only to the neuron whose action was selected\n",
    "            output = np.zeros((2,1))\n",
    "            output[Action,0] = 1\n",
    "\n",
    "            # Move State\n",
    "            stateIndex += Action\n",
    "\n",
    "            # Update weights\n",
    "            dw = learningRate * (rOld - QvalueOld + gamma * Qvalue[Action]) * outputOld.dot(inputOld.T)\n",
    "            weights += dw\n",
    "\n",
    "            # Update variables for sarsa\n",
    "            QvalueOld = Qvalue[Action]\n",
    "            outputOld = output\n",
    "            inputOld = input_vector\n",
    "            rOld = r\n",
    "\n",
    "        # Update weights for the terminal state\n",
    "        dw = learningRate * (rOld - QvalueOld) * outputOld.dot(inputOld.T)\n",
    "        weights += dw\n",
    "\n",
    "        # Store reward for the current trial\n",
    "        Rewards[0,trial] = rOld\n",
    "\n",
    "    return Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script that calls the function sarsa_monkey_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Parameter setup\n",
    "nTrials = 20        # should be integer >0\n",
    "learningRate = 0.8  # should be real, Greater than 0\n",
    "epsilon = 0.001     # should be real, Greater or Equal to 0; epsilon=0 Greedy, otherwise epsilon-Greedy\n",
    "\n",
    "gamma = 0.9         # should be real, positive, smaller than 1\n",
    "\n",
    "repetitions = 100   # number of episodes, should be integer, greater than 0; for statistical reasons\n",
    "\n",
    "totalRewards = np.zeros((repetitions,nTrials))  # reward matrix. each row contains rewards obtained in one episode\n",
    "\n",
    "fontSize = 18\n",
    "\n",
    "# Start iterations over episodes\n",
    "for j in range(repetitions):\n",
    "    totalRewards[j,:] = sarsa_monkey_nn(nTrials,learningRate,epsilon,gamma)\n",
    "\n",
    "# TODO: Plot the average reward as a function of the number of trials --> the average has to be performed over the episodes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
